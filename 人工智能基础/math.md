# 基础思想

## 二进制

计算机为什么使用二进制:

* 计算机使用二进制和现代计算机系统的硬件是实现有关,组成计算机系统的逻辑电路通常只有两个状态,即开关的连通与断开.0表示断开,1表示接通,每位数据只有断开和接通两种状态,所以抗干扰能力强,可靠性高
* 二进制非常适合逻辑运算.真与假,与或非,加减乘除

向左移动:

* 二进制向左移动一位,就是在末尾添加0
* 数字溢出就是二进制数的位数超过了系统指定的位数
* 二进制左移一位,其实就是将数字翻倍

向右移动:

* 二进制右移一位,就是将数字除以2并求整数商的操作

在java或python中,左移位是`<<`,而右移是`>>>`(逻辑右移),因为二进制数值的最高位是符号位(补码),java里定义了两种右移,逻辑右移和算术右移

* 逻辑右移1位,左边补0即可
* 算术右移保持符号位不变,除符号位之外的右移一位并补符号位1
* C/C++中,逻辑右移和算术右移共享一个运算符`>>`,如果运算数类型为`unsigned`,则逻辑右移,如果是`signed`,则算术右移

逻辑操作:

* 与或非,异或

[CPP位运算](../../01_basic/bit/readbit.md)

## 余数

余数总是在一个固定的范围内,通过某一种关系,让整数处于一个确定的边界内

同余定理:两个整数a和b,如果它们除以正整数m得到的余数相等,我们可以说,a和b对于模m同余.同余定理其实就是用来分类的

哈希:将任意长度的输入,通过哈希算法,压缩为某个固定的输出

余数的应用:散列函数,加密算法,循环冗余校验等等

## 迭代法

迭代法:用旧的变量值,递推计算新的变量值

迭代法的基本步骤:

* 确定用于迭代的变量
* 确立迭代变量之间的递推关系
* 控制迭代的过程

迭代法的具体应用:

* 求数值的精确或是近似解
* 在一定范围内查找目标值
* 机器学习算法中的迭代

## 数学归纳法

数学归纳法:从理论上证明某个结论

数学归纳法的一般步骤:

* 证明基本情况(n=1)是否成立
* 假设n=k-1成立,再证明n=k也成立

和使用迭代法的计算相比,数学归纳法最大的特点就在于归纳二字,它已经总结出了规律,只要我们能够证明这个规律是正确的,就没必要进行逐步的推算

递归调用的代码和数学归纳法的逻辑是一致的(递归把计算交给计算机,归纳把计算交给人)

## 递归

在递归中,每次嵌套调用都会让函数体生成自己的局部变量,可以用来保存不同状态下的数值,可以省去大量中间变量的操作,极大地方便了设计和编程

递归就是将复杂的问题,每次都解决一点点,并将剩下地任务转化为更简单地问题等待下次求解,如此反复,直到最简单的形式

递归和循环都是迭代法的实现,在某些场合下,它们的实现可以相互转化

分而治之:将一个复杂的问题,分解成两个甚至多个规模相同或类似的子问题,然后对这些子问题再进一步细分,直到最后的子问题变得很简单

分布式系统

## 排列组合

排列是从从n个不同的元素中取出m个不同的元素,按照一定的顺序排列成一列,这个过程叫排列,当m=n时,称为全排列

排列可以穷举出随机变量取值的所有可能性,如暴力破解法

组合是从n个不同的元素中取出m个不同的元素,所有m取值的组合的全集合,称为全组合

## 动态规划

动态规划:通过不断分解问题,可以将复杂的任务简化为最基本的小问题,我们需要再各种可能的局部解中,找出可能达到最优的局部解,这个寻找最优解的过程就是动态规划

动态规划的关键是子问题之间的转移关系,也称为状态转移方程,找到合适的状态转移方程,是动态规划的关键

状态转移方程:从上一个状态到下一个状态之间可能存在的一些变化,以及基于这些变化的最终决策结果,这样的表达式称为状态转移方程

## 反码和补码

如何让计算机理解哪些是正数,哪些是负数?

如果是有符号数,那么最高位是符号位,当符号位为0时,表示数值为正数,当符号位为1时,该数值为负数

Java中所有和数字相关的数据类型都是有符号的,而C/C++中,用unsigned定义无符号位的数据类型

溢出:计算机系统中,总有一个物理上的极限,一旦某个数字超过了这些限定,就会法生溢出,如果超出了上限,则上溢出,如果超出了下限,则下溢出

计算机数据的溢出,就相当于取模:

* 上溢出后,又从下限开始,最大的数值加1,就变成了最小的数值,类似于余数和取模的概念.取模的除数就是数据类型的上限减去下限的值再加上1A
* ![image-20201025153153974](img\49.png)

原码:最高位是符号位,其余位用来表示该数字绝对值的二进制,无法直接使用负数的原码来进行减法计算

利用溢出取模,对计算机里的减法进行变换,假设`i-j`,其中`j`为正数.如果`i-j`加上取模的除数,`i-j = ( i-j ) + ( 2^n -1+1) = i + (2^n -1-j+1)`

![image-20201025154341828](img\50.png)

`2^n -1-j`相当于对正数`j`的二进制原码除了符号位之外按位取反. 由于负数`-j`和正数`j`的原码除了符号位之外都是相同的,所以`2^n -1-j`也相当于对负数`-j`的二进制原码除了符号位之外按位取反.我们把`2^n -1-j`所对应的编码称为负数`-j`的反码

所以 `i-j = i + (2^n -1-j+1) = i的原码 + (-j的反码)+1`

把`(-j的反码)+1`定义为`-j`的补码,所以`i-j = i的原码 + (-j的补码)`

而正数的原码反码补码都是一样的,所以`i-j = i的补码 + (-j的补码)`

# 概率统计

概率统计关注的是随机变量及其概率分布,以及如何通过观测数据来推断这些分布,在机器学习,数据挖掘等领域,概率统计发挥着至关重要的作用

## 基本概念

随机变量:描述事件所有可能出现的状态,分为离散型随机变量和连续型随机变量

概率分布:描述每个状态出现的可能性

联合概率:由多个随机变量联合起来才能决定的概率分布

边缘概率:对联合概率P(x,y)在y上求和,可以得到P(x),P(x)称为边缘分布

条件概率:某个事件受其他事件影响之后出现的概率P(x | y)

## 概率基础

概率分布描述就是随机变量的概率分布

伯努利分布:随机变量取值只有0或1

![](img\01.png)

分类分布:随机状态具有k个不同的状态

![](img\02.png)

正态分布:

![](img\03.png)

数学期望:每次随机结果的出现概率乘以其结果的总和

概率,条件概率,联合概率的关系:

![](img\04.png)

贝叶斯定理:用先验概率和似然函数估计后验概率

![](img\05.png)

相互独立:

![](img\06.png)

![](img\07.png)

独立一定不相关,不相关不一定独立,不相关说明两个事件之间不是线性关系,但可能是其它关系,而独立则没有关联

## 朴素贝叶斯

朴素:假设数据对象的不同属性对其归类影响时是相互独立的

贝叶斯公式:c表示一个分类,f表示属性对应的数据字段

![](img\08.png)

`P(c | f)`为待分类的样本,出现属性`f`时,样本属于类别`c`的概率

`P(f | c)`是根据训练数据统计,得到分类`c`中出现属性`f`的概率

`P(c)`为分类`c`在训练数据中出现的概率

`P(f)`是属性`f`在训练样本中出现的概率

当待分类得样本还有多个属性`f`时的朴素贝叶斯

![](img\09.png)

若P过小时,计算机会无法识别,我们可以取`log`将小数转化为绝对值大于1的负数

朴素贝叶斯分类的步骤:

* 准备数据:将样本的常见属性转化为计算机所能理解的数据,这种数据也称为训练样本
* 建立模型:根据样本属性等数据计算先验概率`P(c)`和条件概率`P(f | c)`,这个过程也称为基于样本的训练
* 分类新数据:对一个未知数据根据建立的模型进行推导计算,得到其属于哪个分类的概率,实现分类的目的,这个过程也称为预测

朴素贝叶斯分类和其它算法分类的比较:

* 和KNN最近邻相比,朴素贝叶斯需要更多的事件进行模型的训练,但是它对新的数据进行分类预测的时候,通常效果更好,用时更短
* 和决策树相比,朴素贝叶斯并不能提供一套易于人类理解的规则,但是它可以提供决策树通常无法支持的模糊分类
* 和SVM支持向量机相比,朴素贝叶斯无法直接支持连续值得输入,所以我们需要将属性得连续值转化为离散值,便于朴素贝叶斯进行处理

## 语言模型

**链式法则**:使用一系列条件概率和边缘概率,来推导联合概率

![](img\10.png)

**马尔科夫假设**:任何一词wi出现的概率只和它前面的1或若干个词有关

**多元文法模型**:基于马尔科夫假设,它表示任何一个词出现的概率,只和它前面的N-1个词有关

二元文法模型:某个词出现的概率只和它前面1个单词有关

![](img\11.png)

三元文法模型:某个词出现的概率只和它前面的两个单词有关

![](img\12.png)

一元文法模型:每个词出现都是相互独立的

语言模型的应用:

* 使用联合概率,条件概率和边缘概率的三角关系,进行相互推导,链式法则
* 使用马尔科夫假设,把受较多随机变量的条件概率,简化为受较少随机变量影响的条件概率,甚至是边缘概率
* 使用贝叶斯定理,通过先验概率推导后验概率

## 马尔科夫模型

马尔科夫假设:任何一词wi出现的概率只和它前面的1或若干个词有关

把词抽象为一个状态,状态到状态之间是有关联的,前一个状态有一定概率可以转移到下一个状态,如果多个状态之间的随机转移满足马尔科夫假设,那么这类随机过程就是一个马尔科夫随机过程,而刻画这类随机过程的统计模型,就是**马尔科夫模型**

如果一个状态出现的概率只和前一个状态,那么称之为一阶马尔科夫模型或马尔科夫链

Google的PageRank链接算法的核心思想就是基于马尔科夫链,状态表示不同的网页,状态之间的转移就代表了人们在不同网页之间的跳转

如果我们无法确定马尔科夫过程中某个状态的取值,这称为隐马尔科夫模型

**隐马尔科夫模型**分为两层,一层是我们可以预测到的数据,称为输出层,另一层是我们无法直接预测到的状态,称为隐藏状态层,同时考虑了状态之间转移概率和状态产生输出的概率,为语音识别,手写识别,机器翻译等提供了可行的解决方案

![](img\13.png)

x1,x2,x3属于隐藏状态层,a为转移概率,b为输出概率,y为输出层

隐藏状态层产生输出层的概率:

![](img\14.png)

我们可以把两层模型看作图的结构,状态和输出是结点,转移和输出关系是边,相应概率是边得权重,这时我们可以对Dijkstra算法稍加修改,来找出权重乘积最大的最优路径,提升查找的效率.利用状态之间存在的先后关系,使用基于动态规划的维特比(Viterbi)算法来找出最优路径

## 信息熵

**信息熵**是衡量给定集合的信息简单或复杂,纯净或混乱的一个指标

![](img\15.png)

n表示集合中分组的数量,`Pi`表示属于第`i`个分组的元素在集合中出现的概率

将一个集合`P`划分为多个更小集合`P1,P2...`后熵的计算,T表示一种划分

![](img\16.png)

将一个集合内的元素区分开来,划分为不同的小组,划分后的整体熵小于划分之前的整体熵,降低划分后每个小集合的混乱程度,也就是降低它们的熵,我们将划分后整体熵的下降,称为**信息增益**

![](img\17.png)

对于一道测试题,信息增益越大则其区分力越强

## 决策树

训练决策树的基本思想:

* 根据集合中的样本类型,为每个集合计算信息熵,并通过全部集合的熵值加权平均,获得整个数据集的熵.一开始集合只有一个,包含所有的样本
* 根据信息增益,计算每个特征的区分能力,挑选区分能力最强的特征,对每个集合进行更细的划分
* 划分不同类型的集合后,重复一二步,直到没有更多的特征或样本全部分号类

因为每个样本可能属于不同的分类,所以决策树通常只能把整体的熵降到一个比较低的值,所以训练得到的决策树模型,常常无法完全正确地划分训练样本,只能求得一个近似解

几种决策树算法的异同:

* 采用信息熵来构建决策树的算法称为[ID3](https://zh.wikipedia.org/wiki/ID3%E7%AE%97%E6%B3%95)(迭代二叉树3代),此算法一般优先考虑具有较多取值的特征,因为取值多的特征会有相对较大的信息增益,因为更多的取值会把样本划分为更多更小的分组,从而有较高的信息增益,但是这样很容易导致机器学习中的过拟合现象,不利于决策树对新数据的预测
* [C4.5算法](https://zh.wikipedia.org/wiki/C4.5%E7%AE%97%E6%B3%95),使用信息增益率来代替信息增益作为选择特征的标准,降低决策树过拟合的程度
* CART算法(Classification and Regression Trees,分类与回归树)
  * 不再采用信息增益或是信息增益率,而是采用基尼指数选择更好的特征进行数据的划分
  * 采用二叉树,每次把数据切成两份

## 特征选择

**特征选择**尝试发掘和预定义任务相关的特征,同时过滤不必要的噪声特征,减少数据类型和维度,降低机器学习的难度,提供模型的准确度,它主要包括特征子集的产生,搜索和评估

一个好的特征选择,可以把那些对分类有价值的信息提取出来而过滤那些对分类没有价值的信息,可以利用**信息熵**和**信息增益**来进行**特征选择**

* 如果熵值很低,说明包含这个特征的数据只出现在少数类中,对分类的判断有价值.计算每个特征所对应的数据集之熵,可以按照熵值由低到高对特征进行排序,选择排列靠前的特征

* 如果基于某个特征的划分,所产生的信息增益越大,说明这个特征对于分类的判断越有价值.我们可以计算基于每个特征的划分,所产生的信息增益,按照信息增益由高到低对特征进行排序,挑选出排列靠前的特征

利用卡方检验进行特征选择,检验特征与分类两个变量是否独立:

* 如果两者独立,则特征和分类没有明显的相关性,反之两者具有较强的相关性
* 如果特征和分类的相关性很高,说明正向相关值远大于负向相关值,或负向相关值远大于正向相关值
* 如果特征与分类相关性很低,说明正向相关值和负向相关值很接近
* 按照卡方检验的值由高到低对特征进行排序,挑选出排列靠前的特征

## 特征变换

数值变换:使用统计中的数据分布,对连续型的数值进行转换,让多个特征的结合更有效

两种常见的特征变换方法:归一化和标准化

**归一化**:获取原始数据的最大值和最小值,然后把原始值线性变换到`[0,1]`之间

![](img\18.png)

* x为原始值,max为样本数据的最大值,min为样本数据的最小值
* 但是只考虑了最值,而最大值与最小值非常容易受到噪音数据的影响

**标准化**:基于正态分布的z分数标准化,假设数据呈现标准正太分布

![](img\19.png)

* x为原始值,μ为均差,σ为标准差
* 这样变换之后,高于平均数的分数会得到一个正的标准分,低于平均数的分数会得到一个负的标准分数,且转换后的数据符合标准状态分布,以0为均值,1为标准差
* 和归一化相比,标准化使用了数据是正态分布的假设,不容易受到过大或过小值得干扰

## 差异显著性检验

A/B测试:同一个目标制定两个或多个方案,让一部分用户使用A方案,另一部分用户使用B方案,记录下每个部分用户的使用情况,看哪个方案产生的结果更好

在A/B测试中,我们所得到得测试结果,实际上只是一种采样,不能简单地根据平均值来判断哪个方案更优

**显著性差异**:研究多组数据之间的差异,是由于数据分布导致还有由于采样的误差导致

* 有显著性差异:两个分布之间的差异
* 无显著性差异:采样引起的差异
* 显著性差异是指不同的组可能来自不同的数据分布,而显著差异是指差异的幅度很大,两者没有必然的联系

**统计假设检验**:事先对随机变量的参数或总体分布做出一个假设,然后利用样本信息来判断这个假设是否合理.这种假设称为**虚无假设**,通常记作H0.与虚无假设对立的假设称为**对立假设**,记作H1

* 先认为原假设H0成立,判断假设是否合理,若产生了小概率事件,则拒绝H0,接受H1,否则接受原假设
* 这个小概率事件的概率记为α,称为显著性水平,通常不超过0.05

**显著性检验**是统计假设检验的一种,它可以帮助我们判断多组数据之间的差异,是采样导致的偶然,还是由于不同数据分布导致的必然

* 假设多个数据之间没有差异(没有显著性)
* 如果样本发生的概率小于显著性水平α,则拒绝原假设,说明多个分布之间有差异,否则接受原假设

**P值**(Probability Value):H0假设为真时,样本出现的概率

* 如果P值很小,说明观测值与假设H0的期望值有很大的偏离,H0发生的概率很小,拒绝原假设,P值越小,越应该拒绝原假设
* 只要计算出P值,我们就能把P值和显著性水平α进行比较,从而决定是否接受原假设

## 方差分析

方差分析,也叫F检验,这种方法可以检验两种或者多组样本的均值是否具备显著性差异,它有四个前提假设

* 随机性:样本是随机采样的
* 独立性:来自不同组的样本是相互独立的
* 正态分布性:组内样本都是来自一个正态分布
* **方差齐性**:不同组的方差相等或相近

SST表示所有采样数据的因变量方差(Total Sum of Squares)

![](img\21.png)

SSM表时数据分布所引起的方差,称为模型平方和(Sum of Squares for Model)

![](img\22.png)

SSE表时采样引起的方差,称为误差平方和(Sum of Squares for Error)

![](img\23.png)

* SST=SSM+SSE
* 如果SSM占比更大,说明**因素**对因变量具有显著的影响
* 如果SSE占比更大,说明**采样误差**对因变量的差异具有显著的影响

![](img\20.png)

* `s`为水平(因素)的个数,`n`为所有样本的总数量,`s-1`为分布的自由度,`n-s`为误差的自由度
* F值越大,说明`SSM/(s-1)`越大,即因素对因变量具有显著影响,差异具**有显著性**
* F值越小,说明`SSE/(n-s)`越大,即采样误差对因变量的差异具有显著的影响,**差异没有显著性**

对于两个算法的比较,如a算法的测试结果的平均值要优于b算法,如果差异没有显著性,说明这个"优"的偶然性很大,并不意味着算法a比算法b更好

方差分析要求因变量属于正态分布,并且具有方差齐性,如果因变量的分布明显是非正态,或者方差的差异很显著,那么不能直接使用方差分析

* 对于方差不齐的情况,可以选择适当的函数对原始数据进行转换,或剔除某些数据,直到方差齐性变得显著
* 对于非正态分布的数据,我们可以使用非参数的分析,非参数检测是在总体的方差知道很少的情况下,利用样本数据对总体分布形态等进行推断的方法,常见的非参数检验包括二项分布检验,K-S检验,卡方检验等等

## 模型拟合

模型拟合:在监督式学习中训练一个模型,更学术地描述应该拟合一个模型,通过模型的假设和训练样本,推导出具体参数的过程

欠拟合:拟合得到的模型过于简单,和训练样本之间的误差非常大,这种拟合模型和训练样本之间的差异,称为偏差

过拟合:拟合得到的模型非常精细和复杂,和训练样本之间的误差非常小,这样的模型缺乏泛化的能力,无法很好地处理新的数据

![](img\24.png)

* 欠拟合产生的原因是特征维度过少,拟合的模型不够复杂,无法满足训练样本,最终导致误差较大,因此我们可以增加特征维度,让输入的训练样本具有更强的表达能力

* 过拟合的问题主要原因则是特征维度过多,导致拟合的模型过于完美地符合训练样本,但是无法适应测试样本或新数据.所以我们可以减少特征的维度,增加训练样本的数据量,并尽量保持训练数据和测试数据分布的一致性
  * 决策树算法容易导致过拟合,使用剪枝和随机森林来缓解过拟合
  * 当我们拥有大量的训练数据时,使用交叉验证的划分方式来保持训练数据和测试数据的一致性

## 概率统计总结

概率统计学习了很多概念

随机变量和它的概率分布体现了事物发生的不确定性,而条件概率、联合概率和边缘概率体现了多个随机变量之间的关系以及是不是相互独立，通过这三者的关系，可以推导出贝叶斯定理。

在贝叶斯定理和变量独立性假设的基础之上，推导出朴素贝叶斯算法中的公式，以及如何使用先验概率来预测后验概率。由于朴素贝叶斯假定多个变量之间相互独立，因此特别适合特征维度很多、特征向量和矩阵很稀疏的场景。基于词包方法的文本分类就是个非常典型的例子。

文本分类涉及了词与词之间相互独立的假设，然后延伸出多元文法，而多元文法也广泛应用在概率语言模型中。语言模型是马尔科夫模型的一种，而隐马尔科夫模型在马尔科夫模型的基础之上，提出了两层的结构，解决了我们无法直接观测到转移状态的问题。

由概率知识派生而来的信息论，也能帮助我们设计机器学习的算法，比如决策树和特征选择。统计中的数据分布可以让特征值转换更合理，而假设检验可以告诉我们哪些结论是更可靠的。

由于很多监督式学习都是基于概率统计的，概率和统计可以帮助机器学习中算法学习训练样本中的特征，并可以对新的数据进行预测，这就是模型拟合的过程。

# 线性代数

有些问题,我们不仅要关心单个变量之间的关系,还要进一步研究多个变量之间的关系,比如基于多个特征的信息检索和机器学习在研究多个变量之间关系的时候,线性代数成为了解决这类问题的有力工具,

## 基本概念

标量只是一个单独的数字,并且不能表示方向

向量代表一组数字,并且这些数字是有序排列,除了拥有数值的大小,还拥有方向

两个向量之间的加法,需要维度相同,对应的元素相加

向量之间的乘法默认是点乘,点乘的作用是把两个向量转换成了标量

矩阵由多个长度相等的向量组成

矩阵相乘:X的行向量与Y的列向量两两点乘

![](img\25.png)

矩阵转置:矩阵内的元素行索引和纵索引互换,就是原矩阵以对角线为轴进行翻转后的结果

![](img\26.png)

方阵:行数和列数相等

单位矩阵:所有沿主对角线的元素都是1,其它位置的所有元素都是0

逆矩阵:矩阵X的逆矩阵记作X^-1^,两者相乘的结果是单位矩阵

特征向量:我们使用向量来表示某个物体的特征,向量的每个元素代表一维特征,而元素的值代表了相应特征的值

矩阵的特征向量:矩阵几何意义是坐标的变换,矩阵的特征向量表示了它在空间中最主要的运动方向

## 向量空间模型

机器学习中的监督式学习,重点考察了特征和分类标签之间的关系,但是在信息检索和非监督学习中,我们更关注的是不同事物之间的相似程度,这就需要用到线性代数中的向量空间模型

F是一个实数域,Fn是由F上所有n维向量构成的集合,假设 V 是 Fn 的非零子集，如果对任意的向量 x、向量 y∈V，都有 (x+y)∈V，我们称为 V 对向量的加法封闭；对任意的标量 k∈V，向量 x∈V，都有 kx 属于 V，我们称 V 对标量与向量的乘法封闭。如果 V 满足向量的加法和乘法封闭性，我们就称 V 是 F 上的**向量空间**。

**向量之间的距离**:如果把一个向量想象为n为空间中的一个点,而向量空间中的两个向量的距离,就是这两个向量所对应的点之间的距离

曼哈顿距离:A点到B点有多条路径,但无论哪条,曼哈顿距离都是一样的

* ![](img\27.png)
* `n`表时向量维度,`xi`表示第一个向量的第`i`维元素的值,`yi`表示第二个向量第`i`维元素的值

欧式距离:欧几里得距离,指n维空间中两个点之间的真实距离

* ![](img\28.png)

切比雪夫距离:

* ![](img\29.png)

闵氏距离:前三个距离的通用表达形式

**向量的长度**:也叫向量的模,是向量所对应点到空间原点的距离,通常使用欧氏距离表示

* 范数满足非负性,齐次性和三角不等式,常常被用来衡量某个向量空间中向量的大小或者长度
* L1范数 ||x|| ,它是x向量各个元素绝对值之和,对应于向量x和原点之间的曼哈顿距离
* L2范数 ||x||~2~,它是x向量各个元素平方和的1/2次方,对应于向量x和原点之间的欧氏距离

**向量之间的夹角**:

* ![](img\30.png)
* 分子是两个向量的点乘,分母是两者长度的乘积
* 余弦值越大,说明两个向量夹角越小,两点相距越近,值越小,说明夹角越大,两点相距越远

**向量的空间模型**:向量的空间模型假设所有的对象都可以转化为向量,然后使用向量间的距离或者向量间的夹角余弦来表示两个对象之间的相似程度

* 夹角余弦的取值在-1到1之间,而且越大表示越相似,所以可以直接作为相似度的取值,欧式距离的范围可能很大,而且与相似度呈现反比关系,所以通常需要进行`1/(ED+1)`这种归一化
* 应用:信息检索领域,基于相似度的机器学习算法,如K近邻(KNN)分类,K均值(K-Means)聚类等

## 信息检索

信息检索就是让计算机根据用户信息需求,从大规模,非结构化的数据中,找出相关的资料

信息检索中的向量空间模型:

* 将文档集合都转换成向量的形式
* 把用户输入的查询转成向量的形式,然后把这个查询的向量和所有文档的向量,进行比对,计算基于距离或者夹角余弦的相似度
* 根据查询和每个文档的相似度,找出相似度最高的文档,认为它们是和指定查询最相关的
* 评估查询结果的相关性

把文档转为特征向量:

* 任何向量都有两个主要的构成要素:维度和取值.维度表示向量有多少维分量,每个分量的含义是什么,取值表示每个分量的数值是多少
* 向量空间模型把字典中每个词条作为向量的一个维度
* 维度的取值可以是"1"或"0",但要考虑每个词的权重,通常使用词频和词频x逆文档频率来实现

查询和文档的匹配:

* 首先将查询转换为向量
* 将查询的向量和所有文档的向量以此对比(计算向量之间的距离或者夹角余弦)

排序和评估:

* 我们按照和输入查询的相似程度,对所有文档进行相似度由高到低的排序,然后取出前面的若干个文档,作为相关的信息返回
* 我们还需要设计各种离线或者在线的评估,来衡量向量空间模型的效果

## 文本聚类

在实际场景中,有时候不存在任何样本的先验知识,而是需要机器在没人指导的情形下,去将很多东西进行归类.由于缺乏训练样本,这种学习称为非监督学习,也就是通常所说的聚类.在这种学习体系中,系统必须通过一种有效的方法发现样本的内在相似性,并把数据对象以群组的形式进行划分

K均值( K-Means)聚类算法,它让我们可以在一个任意多的数据上,得到一个事先定好群组数量(K)的聚类结果.核心思想:尽量最大化总的群组内相似度,同时尽量最小化群组之间的相似度.群组内或群组间的相似度,是通过各个成员和群组质心相比较来确定的

* 从N个数据对象中随机选取k个对象作为质心,质心可以是群内所有成员对象的平均值
* 对剩余的对象,测量它和每个质心的相似度,并把它归到最近的质心所属的群组
* 重新计算已经得到的各个群组的质心.计算质心是关键,如果使用特征向量来表示数据对象,那么最基本的方法是取群组内成员的特征向量,将它们的平均值作为质心的向量表示
* 重复二三步,直至新的质心和原质心相等或相差值小于指定阈值,算法结束

使用向量空间对新闻进行聚类:将内容非常相似的文章聚类到同一个分组

* 将文档集合都转换成向量形式
* 使用K均值算法对文档集合进行聚类,关键是如何确定数据对象和分组质心之间的相似度
  * 使用向量空间中的距离或者夹角余弦度量,计算两个向量的相似度
  * 计算质心的向量,质心是分组里成员的平均值
* 在每个分类中,选出和质心最接近的几篇文章作为代表,而其它文章作为冗余的内容过滤掉

## 矩阵

矩阵由多个长度相等的向量组成,其中每列或者每行就是一个向量

### PageRank计算

PageRank是基于马尔科夫链,它假设了一个"随机冲浪者"模型,冲浪者从某张网页出发,根据web图中的链接关系随机访问

![](img\31.png)

`Pi`表示第`i`张网页,`Mi`是`Pi`的入链接集合,`Pj`是`Mi`集合中的第`j`张网页,`PR(pj)`表示网页`Pj`的PageRank得分,`L(pj)`表示网页`Pj`的出链接数量,`1/L(pj)`表示从网页`Pj`跳转到`Pi`的概率.`α`是用户不进行随机跳转的概率,`N`表示所有网页的数量

如果只考虑用户按照网页间链接进行随机冲浪,那么PageRank公式就简化为:

![](img\32.png)

这与矩阵的点乘计算公式基本一致

![image-20201024133934842](img\33.png)

考虑随机跳转,我们可以把PageRank公式简化为矩阵的点乘

![image-20201024134754751](img\34.png)

### 协同过滤推荐

协同过滤是经典的推荐算法之一,它充分利用了用户和物品之间已知的关系,为用户提供新的推荐内容

用矩阵实现推荐系统的核心思想:推荐系统会根据用户所处的场景和个人喜好,推荐他们可能感兴趣的信息和商品

![image-20201024135235255](img\35.png)

第`i`行就是第`i`个用户的数据,第`j`列就是用户对第`j`格物品的喜好程度

协同过滤:利用已有用户群体过去的行为或意见,预测当前用户最可能喜欢哪些东西

基于用户的过滤:指定一个用户访问物品的数据集合,找出和当前用户历史行为有相似偏好的其他用户,将这些用户组成"近邻",对于当前用户没有访问过的物品,利用其近邻的访问记录来预测

基于物品的过滤:利用物品相似度来计算预测值

## 线性回归

回归分类:

* 按照自变量数量,当自变量x的个数大于1时就是多元回归
* 按照因变量数量,当因变量y的个数大于1时就是多重回归
* 按照模型种类,如果因变量和自变量为线性关系时,就是线性回归模型,如果因变量和自变量为非线性关系时,就是非线性回归模型

### 高斯消元法

高斯消元法主要分为两步:消元和回归,消元就是减少某些方程中元的数量,回归就是把已知解代到方程式中,求出其它未知的解

使用矩阵实现高斯消元法:

* 首先把方程中的系数转成矩阵
* 通过消元,把原始的系数矩阵变为上三角矩阵
* 通过回归,把上三角矩阵变为单位矩阵

增广矩阵:把方程式等号右边的值加入到系数矩阵,称为增广矩阵

* 对于一个增广矩阵,把除了最后一列之外的部分,变成单位矩阵,此时最后一列中的每个值,就是每个自变量所对应的解

利用矩阵相乘进行消元

### 直线拟合

现实中的数据一定存在由于各种各样原因所导致的误差,因此即使自变量和因变量之间存在线性关系,也基本上不可能完美符合线性关系

线性回归分析并不一定需要100%精确,误差ε可以帮助我们降低对精度的要求

为了实现最小化ε,可以使用最小二乘法进行直线拟合,最小二乘法通过最小化误差的平方和,来寻找和观测数据匹配的最佳函数

最小二乘法:通过向量空间的欧式距离之平方,定义了预测值和真实值之间的误差

![image-20201024150755267](img\36.png)

`yi`表示来自数据样本的观测值,`y^`是假设的函数的理论值,ε是误差

最小二乘法公式的矩阵形式:

![image-20201024151158859](img\37.png)

B为系数矩阵,X为自变量矩阵,Y为因变量矩阵

B的最终化简结果:

![image-20201024151930451](img\38.png)

## PCA主成分分析

在机器学习领域,我们要进行大量的特征工程,把物品的特征转换成为计算机所能处理的各种数据.

随着特征数量不断地增加,特征向量的维度也会不断上升,这不仅会加大机器学习的难度,还会影响最终的准确度,针对这种情况,我们需要过滤掉一些不重要的特征,或者把某些相关的特征合并起来,最终达到减少特征维度的同时,尽量保留原始数据所包含的信息

PCA( Principal Component Analysis )的主要步骤:

* 标准化样本矩阵中的原始数据
  * ![image-20201024173904185](img\39.png)
* 获取标准化数据的协方差矩阵
  * 协方差用于衡量两个变量的总体误差
  * ![image-20201024174014038](img\40.png)
  * 协方差矩阵,矩阵第`i`列向量和第`j`列向量之间的协方差
  * ![image-20201024174346769](img\41.png)
* 计算协方差矩阵的特征值和特征向量
  * 对于一个矩阵X,如果能找到向量v和标量λ,使Xv=λv,则v是矩阵X的特征向量,λ是矩阵X的特征值
  * 把向量v左乘一个矩阵X看做对v进行旋转或拉伸,而这种旋转和拉伸都是由于左乘矩阵X后所产生运动所导致的,特征向量v表示矩阵X运动的方向,特征值λ表示了运动的幅度,这两者结合就能描述左乘矩阵X所带来的效果,因此被看作矩阵的特征
  * PCA中的主成分就是指特征向量,而对应的特征值的大小,就表示这个特征向量或者说主成分的重要程度,特征值越大,重要程度越高,要优先这个主成分,并利用这个主成分对原始数据进行变换
* 依照特征值的大小,挑选主要的特征向量
  * 通过公式可以获得k个特征值和对应的特征向量,按照对应λ数值的大小,排名靠前的v就是最重要的特征向量
  * 取前k1个重要的特征,使用者k1个特征向量,组成n*k1维的矩阵D
* 生成新的特征
  * 把包含原数据的 m * n 维的矩阵X左乘矩阵D,就能重新获得一个m * k1维的矩阵,达到了降维的目的

## SVD奇异值分解

奇异值分解是另一种降维的方法

PCA是通过分析不同维度特征之间的协方差,找到包含最多信息量的特征向量,从而实现降维,而SVD( Singular Value Decomposition )试图通过样本矩阵本身的分解,找到一些潜在的因素,然后通过把原始的特征维度映射到较少的潜在因素上,达到降维的目的

方阵的特征分解:

* 方阵:行数和列数相等的矩阵,酉矩阵:矩阵和其转置矩阵相乘得到的是单位矩阵,只有方阵才能进行有实数解的特征分解
* 特征值和特征向量:
  * 对于一个 n x n 维的矩阵X,n维向量ν,标量λ,如果有Xν=λv,那么λ是X的特征值,ν是X的特征向量,并对应于特征值λ
  * 特征向量表示了矩阵变换的方向,而特征值表示了变化的幅度,通过特征值和特征矩阵,我们可以把矩阵X进行特征分解
* 特征分解:
  * 把矩阵分解为由其特征值和特征向量表示的矩阵之积的方法
  * 矩阵X的k个特征值λ1,λ2,...,λn,以及这n个特征值所对应的特征向量v1,v2,...,vn,那么就有 XV=VΣ ,V是这n个特征向量所张成的 nxn 维矩阵,而Σ是这n个特征值为主对角线的 nxn 维矩阵
  * 经过推导求得 X=VΣV^-1^ ,将V的n个特征向量进行标准化处理,V的n个特征向量为标准正交基,满足 V^t^V=I ,V是酉矩阵,有 V^t^=V^-1^,所以特征分解表达式为 X=VΣV^t^

矩阵的奇异值分解:

* 假设矩阵X是一个 mxn 维的矩阵,那么X的SVD为 X=UΣV^t^
* SVD分解与特征分解类似,但并不要求分解的矩阵为方阵,U和V^t^并不互为逆矩阵.U是 mxm 维德矩阵,V是 nxn 维矩阵,Σ是 mxn 维的矩阵
* Σ只有主对角线上的元素非0,其他元素都是0,主对角线上的每个元素称为奇异值,U和V都是酉矩阵
* 因为X不是方阵,可以把X的转置X^t^和X做矩阵乘法,得到 nxn 维的对称方阵 X^t^X,对这个矩阵做特征分解,它的所有特征向量构成一个 nxn 维的矩阵V,V中的每个特征向量叫作X的右奇异向量
* 而通过矩阵乘法得到 mxm 维的方阵 XX^t^,同样做特征分解,得到m个特征值和对应m个特征向量u,所有特征向量u构成 mxm 的矩阵U,U中的每个特征向量叫作X的左奇异向量

潜在语义分析LSA

最简单的向量空间模型采用的是精确的词条匹配,没有办法处理词条形态的变化,同义词,近义词等情况,需要建立同义词,近义词词典.但是有一些词语并不是同义词1或者近义词,但是相互之间也有语义关系,例如"学生","老师"等

通过潜在语义分析LAS( Latent Semantic Analysis )可以让计算机通过大量的数据,找到词语之间的关系

LSA通过词条和文档所组成的矩阵,发掘词和词之间的语义关系,并过滤掉原始向量空间中存在的一些噪声

* 分析文档集合,建立表示文档和词条关系的矩阵
* 对文档-词条进行SVD奇异值分解,分解之后得到的奇异值a对应一个语义上的概念,而a值得大小表示这个概念在整个文档集合中的重要程度
  * U中的左奇异值向量表示文档和这些语义概念的关系强弱,V中的右奇异值向量表示每个词条和这些语义概念的关系强弱
* 对SVD分解后的矩阵进行降维,这个操作和PCA主成分分析的降维操作类似
* 使用降维后的矩阵重新构建概念-文档矩阵,新矩阵中的元素不再表示词条是不是出现在文档中,二十表示某个概念是不是出现在文档中

## 矩阵乘法的几何意义

正交向量:两个向量的点乘结果为0

一个向量左乘一个矩阵,实际上是对这个向量进行了一次变换

<img src="img\42.png" alt="image-20201025125620391" style="zoom:50%;" />

<img src="img\43.png" alt="image-20201025125654762" style="zoom:50%;" />

矩阵X1是一个对角矩阵,特征值分别为3和2,对应的特征向量是[1,0]和[0,1],在二维坐标中,[1,0]实际上是x轴的方向,而[0,1]实际上是y轴的方向,特征值3对应特征向量[1,0]表明x轴方向拉伸为原来的3倍,特征值2对应特征向量[0,1]表明在y轴方向拉伸2倍

<img src="img\44.png" alt="image-20201025125756063" style="zoom:50%;" />



矩阵的特征向量不一定是x轴和y轴,它们可以是二维空间中任意相互正交的向量

## 线性代数总结

线性代数最基本的概念包括了向量、矩阵以及对应的操作。向量表示了一组数的概念，非常适合表示一个对象的多维特征，因此被广泛的运用在信息检索和机器学习的领域中。而矩阵又包含了多个向量，所以适合表示多个数据对象的集合。同时，矩阵也可以用于表达二维关系，例如网页的邻接矩阵，用户对物品的喜好程度，关键词在文档中的 tf-idf 等等。

由于向量和矩阵的特性，我们可以把它们运用在很多算法和模型之中。向量空间模型定义了向量之间的距离或者余弦夹角，我们可以利用这些指标来衡量数据对象之间的相似程度，并把这种相似程度用于定义查询和文档之间的相关性，或者是文档聚类时的归属关系。矩阵的运算体现了对多个向量同时进行的操作，比如最常见的左乘，就可以用在计算 PageRank 值，协同过滤中的用户或者物品相似度等等。

当然，矩阵的运用还不止计算数据对象之间的关系。最小二乘法的实现、PCA 主成分的分析、SVD 奇异值的分解也可以基于矩阵的运算。这些都可以帮助我们发现不同维度特征之间的关系，并利用这些关系找到哪些特征更为重要，选择或者创建更为重要的特征。

有的时候，线性代数涉及的公式和推导比较繁琐。在思考的过程中，我们可以把矩阵的操作简化为向量之间的操作，而把向量之间的操作简化为多个变量之间的运算。另外，我们可以多结合实际的案例，结合几何空间、动手推算，甚至可以编程实现某些关键的模块，这些都有利于理解和记忆。

# 综合应用

离散数学是基础数据结构和编程算法的基石,而概率统计论和线性代数,是很多信息检索和机器学习算法的核心

## 缓存系统

缓存即数据交换的缓冲区,它的读取速度远远高于普通存储介质,可以帮助系统更快地运行

缓存设计的考虑因素:

* 硬件的性能
* 命中率,从高速介质中读取数据,称为"命中",高速介质成本是非常昂贵的,一般不支持持久化,数据容量必须受到限制,一定有部分数据无法在缓存中读取,必须到原始的存储中查找,这种情况称为"错过"
  * 缓存中查找到数据的次数H除以整体的数据访问次数V计算命中率,如果命中率高,系统能够频繁地获取已经在缓存中驻留的数据,速度会明显提升
  * 我们需要使用缓存淘汰算法提高命中率
* 更新周期,被访问的数据不会一成不变,对于数据变化速度很快的数据,我们需要将变动主动更新到缓存中,或者让原有内容失效,即使更新数据

缓存淘汰算法:

* 通过某种机制将缓存中可能无用的数据剔除,然后向剔除后空余的空间中补充将来可能会访问的数据,从而提高命中率
* 最少使用LFU策略( Least Frequently Used ),LFU会记录每个缓存对象被使用的频率,并将使用次数最少的对象剔除
* 最久使用LRU策略( Least Recently Used ),LRU会记录每个缓存对象最近使用的时间,并将使用时间点最久远的对象剔除

![image-20201025135716360](img\45.png)

如何设计一个缓存系统?

* 数据结构:哈希表,通过哈希表计算快速定位,加快查找的速度
* LRU最久未使用策略:使用队列,先入队列的元素会优先得到处理,利用队列的特点,很容易找到一次使用时间最久的数据,将最近使用的数据加入到队尾,对头对应的数据即最久未使用的数据

![image-20201025140325683](img\46.png)

## 搜索引擎

搜索引擎考虑因素:

* 检索效率:通过哈希表结构设计的倒排索引,可以大大提升数据对象的检索效率
* 相关性:利用向量空间模型,来衡量文档和用户查询之间的相似程度,确保两者是相关的

搜索引擎的设计框架:

* 文本搜索系统的框架通常包括两个重要模块:离线的预处理和在线的查询

  * 离线预处理包括数据获取,文本预处理,词典和倒排索引的构建,相关性模型的数据统计等.核心为文本预处理和倒排索引
  * 常规的文本预处理指针对文本进行分词,移除停用词,取词干,归一化,扩充同义词和近义词等
  * 倒排索引:把文档集转换为关键词到文档的这种关系,是典型的牺牲空间来换取时间的方法
  * 在线查询一般会使用和离线模块一样的预处理
  * 不同的相关性模型有不同的计算的方法,最简单的布尔模型只需要计算若干匹配条件的交集,向量空间模型VSM则需要计算查询向量和待查文档向量的余弦夹角,而语言模型需要计算匹配条件的贝叶斯概率等等

  ![image-20201025142150563](img\47.png)

倒排索引的设计:

* 数据结构:哈希表,尤其是基于链式地址法的哈希表,哈希的键就是文档词典的某个词条,值就是一个链表,链表是出现这个词条的所有文档之集合,而链表的每一个结点,就表示出现过这个词条的某一篇文档
* ![image-20201025143928825](img\48.png)
* ID字段表示文档的ID,tf字段表示词频,tfidf字段表示词频-逆文档概率,prob表示这个词条在这篇文档出现的条件概率

向量空间模型可以作为文本搜索的相关性模型,但是它的计算需要把查询和所有的文档进行比较,时间复杂度太高,影响了及时性,这个时候,我们可以利用倒排索引,过滤掉绝大部分不包含查询关键词的文档

## 推荐系统

根据协同过滤算法的核心思想,整个系统可以分为三大步骤

* 用户评分的标准化
* 衡量和其他用户后物品之间的相似度
  * 利用公式计算用户之间相似度us和物品之间相似度is
* 根据相似的用户或物品,给出预测的得分p

## 个性化用户画像的设计