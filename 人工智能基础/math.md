# 基础思想

## 二进制

计算机为什么使用二进制:

* 计算机使用二进制和现代计算机系统的硬件是实现有关,组成计算机系统的逻辑电路通常只有两个状态,即开关的连通与断开.0表示断开,1表示接通,每位数据只有断开和接通两种状态,所以抗干扰能力强,可靠性高
* 二进制非常适合逻辑运算.真与假,与或非,加减乘除

向左移动:

* 二进制向左移动一位,就是在末尾添加0
* 数字溢出就是二进制数的位数超过了系统指定的位数
* 二进制左移一位,其实就是将数字翻倍

向右移动:

* 二进制右移一位,就是将数字除以2并求整数商的操作

在java或python中,左移位是`<<`,而右移是`>>>`(逻辑右移),因为二进制数值的最高位是符号位(补码),java里定义了两种右移,逻辑右移和算术右移

* 逻辑右移1位,左边补0即可
* 算术右移保持符号位不变,除符号位之外的右移一位并补符号位1
* C/C++中,逻辑右移和算术右移共享一个运算符`>>`,如果运算数类型为`unsigned`,则逻辑右移,如果是`signed`,则算术右移

逻辑操作:

* 与或非,异或

[CPP位运算](../../01_basic/bit/readbit.md)

## 余数

余数总是在一个固定的范围内,通过某一种关系,让整数处于一个确定的边界内

同余定理:两个整数a和b,如果它们除以正整数m得到的余数相等,我们可以说,a和b对于模m同余.同余定理其实就是用来分类的

哈希:将任意长度的输入,通过哈希算法,压缩为某个固定的输出

余数的应用:散列函数,加密算法,循环冗余校验等等

## 迭代法

迭代法:用旧的变量值,递推计算新的变量值

迭代法的基本步骤:

* 确定用于迭代的变量
* 确立迭代变量之间的递推关系
* 控制迭代的过程

迭代法的具体应用:

* 求数值的精确或是近似解
* 在一定范围内查找目标值
* 机器学习算法中的迭代

## 数学归纳法

数学归纳法:从理论上证明某个结论

数学归纳法的一般步骤:

* 证明基本情况(n=1)是否成立
* 假设n=k-1成立,再证明n=k也成立

和使用迭代法的计算相比,数学归纳法最大的特点就在于归纳二字,它已经总结出了规律,只要我们能够证明这个规律是正确的,就没必要进行逐步的推算

递归调用的代码和数学归纳法的逻辑是一致的(递归把计算交给计算机,归纳把计算交给人)

## 递归

在递归中,每次嵌套调用都会让函数体生成自己的局部变量,可以用来保存不同状态下的数值,可以省去大量中间变量的操作,极大地方便了设计和编程

递归就是将复杂的问题,每次都解决一点点,并将剩下地任务转化为更简单地问题等待下次求解,如此反复,直到最简单的形式

递归和循环都是迭代法的实现,在某些场合下,它们的实现可以相互转化

分而治之:将一个复杂的问题,分解成两个甚至多个规模相同或类似的子问题,然后对这些子问题再进一步细分,直到最后的子问题变得很简单

分布式系统

## 排列组合

排列是从从n个不同的元素中取出m个不同的元素,按照一定的顺序排列成一列,这个过程叫排列,当m=n时,称为全排列

排列可以穷举出随机变量取值的所有可能性,如暴力破解法

组合是从n个不同的元素中取出m个不同的元素,所有m取值的组合的全集合,称为全组合

## 动态规划

动态规划:通过不断分解问题,可以将复杂的任务简化为最基本的小问题,我们需要再各种可能的局部解中,找出可能达到最优的局部解,这个寻找最优解的过程就是动态规划

动态规划的关键是子问题之间的转移关系,也称为状态转移方程,找到合适的状态转移方程,是动态规划的关键

状态转移方程:从上一个状态到下一个状态之间可能存在的一些变化,以及基于这些变化的最终决策结果,这样的表达式称为状态转移方程

# 概率统计

概率统计关注的是随机变量及其概率分布,以及如何通过观测数据来推断这些分布,在机器学习,数据挖掘等领域,概率统计发挥着至关重要的作用

## 基本概念

随机变量:描述事件所有可能出现的状态,分为离散型随机变量和连续型随机变量

概率分布:描述每个状态出现的可能性

联合概率:由多个随机变量联合起来才能决定的概率分布

边缘概率:对联合概率P(x,y)在y上求和,可以得到P(x),P(x)称为边缘分布

条件概率:某个事件受其他事件影响之后出现的概率P(x | y)

## 概率基础

概率分布描述就是随机变量的概率分布

伯努利分布:随机变量取值只有0或1

![](img\01.png)

分类分布:随机状态具有k个不同的状态

![](img\02.png)

正态分布:

![](img\03.png)

数学期望:每次随机结果的出现概率乘以其结果的总和

概率,条件概率,联合概率的关系:

![](img\04.png)

贝叶斯定理:用先验概率和似然函数估计后验概率

![](img\05.png)

相互独立:

![](img\06.png)

![](img\07.png)

独立一定不相关,不相关不一定独立,不相关说明两个事件之间不是线性关系,但可能是其它关系,而独立则没有关联

## 朴素贝叶斯

朴素:假设数据对象的不同属性对其归类影响时是相互独立的

贝叶斯公式:c表示一个分类,f表示属性对应的数据字段

![](img\08.png)

`P(c | f)`为待分类的样本,出现属性`f`时,样本属于类别`c`的概率

`P(f | c)`是根据训练数据统计,得到分类`c`中出现属性`f`的概率

`P(c)`为分类`c`在训练数据中出现的概率

`P(f)`是属性`f`在训练样本中出现的概率

当待分类得样本还有多个属性`f`时的朴素贝叶斯

![](img\09.png)

若P过小时,计算机会无法识别,我们可以取`log`将小数转化为绝对值大于1的负数

朴素贝叶斯分类的步骤:

* 准备数据:将样本的常见属性转化为计算机所能理解的数据,这种数据也称为训练样本
* 建立模型:根据样本属性等数据计算先验概率`P(c)`和条件概率`P(f | c)`,这个过程也称为基于样本的训练
* 分类新数据:对一个未知数据根据建立的模型进行推导计算,得到其属于哪个分类的概率,实现分类的目的,这个过程也称为预测

朴素贝叶斯分类和其它算法分类的比较:

* 和KNN最近邻相比,朴素贝叶斯需要更多的事件进行模型的训练,但是它对新的数据进行分类预测的时候,通常效果更好,用时更短
* 和决策树相比,朴素贝叶斯并不能提供一套易于人类理解的规则,但是它可以提供决策树通常无法支持的模糊分类
* 和SVM支持向量机相比,朴素贝叶斯无法直接支持连续值得输入,所以我们需要将属性得连续值转化为离散值,便于朴素贝叶斯进行处理

## 语言模型

**链式法则**:使用一系列条件概率和边缘概率,来推导联合概率

![](img\10.png)

**马尔科夫假设**:任何一词wi出现的概率只和它前面的1或若干个词有关

**多元文法模型**:基于马尔科夫假设,它表示任何一个词出现的概率,只和它前面的N-1个词有关

二元文法模型:某个词出现的概率只和它前面1个单词有关

![](img\11.png)

三元文法模型:某个词出现的概率只和它前面的两个单词有关

![](img\12.png)

一元文法模型:每个词出现都是相互独立的

语言模型的应用:

* 使用联合概率,条件概率和边缘概率的三角关系,进行相互推导,链式法则
* 使用马尔科夫假设,把受较多随机变量的条件概率,简化为受较少随机变量影响的条件概率,甚至是边缘概率
* 使用贝叶斯定理,通过先验概率推导后验概率

## 马尔科夫模型

马尔科夫假设:任何一词wi出现的概率只和它前面的1或若干个词有关

把词抽象为一个状态,状态到状态之间是有关联的,前一个状态有一定概率可以转移到下一个状态,如果多个状态之间的随机转移满足马尔科夫假设,那么这类随机过程就是一个马尔科夫随机过程,而刻画这类随机过程的统计模型,就是**马尔科夫模型**

如果一个状态出现的概率只和前一个状态,那么称之为一阶马尔科夫模型或马尔科夫链

Google的PageRank链接算法的核心思想就是基于马尔科夫链,状态表示不同的网页,状态之间的转移就代表了人们在不同网页之间的跳转

如果我们无法确定马尔科夫过程中某个状态的取值,这称为隐马尔科夫模型

**隐马尔科夫模型**分为两层,一层是我们可以预测到的数据,称为输出层,另一层是我们无法直接预测到的状态,称为隐藏状态层,同时考虑了状态之间转移概率和状态产生输出的概率,为语音识别,手写识别,机器翻译等提供了可行的解决方案

![](img\13.png)

x1,x2,x3属于隐藏状态层,a为转移概率,b为输出概率,y为输出层

隐藏状态层产生输出层的概率:

![](img\14.png)

我们可以把两层模型看作图的结构,状态和输出是结点,转移和输出关系是边,相应概率是边得权重,这时我们可以对Dijkstra算法稍加修改,来找出权重乘积最大的最优路径,提升查找的效率.利用状态之间存在的先后关系,使用基于动态规划的维特比(Viterbi)算法来找出最优路径

## 信息熵

**信息熵**是衡量给定集合的信息简单或复杂,纯净或混乱的一个指标

![](img\15.png)

n表示集合中分组的数量,`Pi`表示属于第`i`个分组的元素在集合中出现的概率

将一个集合`P`划分为多个更小集合`P1,P2...`后熵的计算,T表示一种划分

![](img\16.png)

将一个集合内的元素区分开来,划分为不同的小组,划分后的整体熵小于划分之前的整体熵,降低划分后每个小集合的混乱程度,也就是降低它们的熵,我们将划分后整体熵的下降,称为**信息增益**

![](img\17.png)

对于一道测试题,信息增益越大则其区分力越强

## 决策树

训练决策树的基本思想:

* 根据集合中的样本类型,为每个集合计算信息熵,并通过全部集合的熵值加权平均,获得整个数据集的熵.一开始集合只有一个,包含所有的样本
* 根据信息增益,计算每个特征的区分能力,挑选区分能力最强的特征,对每个集合进行更细的划分
* 划分不同类型的集合后,重复一二步,直到没有更多的特征或样本全部分号类

因为每个样本可能属于不同的分类,所以决策树通常只能把整体的熵降到一个比较低的值,所以训练得到的决策树模型,常常无法完全正确地划分训练样本,只能求得一个近似解

几种决策树算法的异同:

* 采用信息熵来构建决策树的算法称为[ID3](https://zh.wikipedia.org/wiki/ID3%E7%AE%97%E6%B3%95)(迭代二叉树3代),此算法一般优先考虑具有较多取值的特征,因为取值多的特征会有相对较大的信息增益,因为更多的取值会把样本划分为更多更小的分组,从而有较高的信息增益,但是这样很容易导致机器学习中的过拟合现象,不利于决策树对新数据的预测
* [C4.5算法](https://zh.wikipedia.org/wiki/C4.5%E7%AE%97%E6%B3%95),使用信息增益率来代替信息增益作为选择特征的标准,降低决策树过拟合的程度
* CART算法(Classification and Regression Trees,分类与回归树)
  * 不再采用信息增益或是信息增益率,而是采用基尼指数选择更好的特征进行数据的划分
  * 采用二叉树,每次把数据切成两份

## 特征选择

**特征选择**尝试发掘和预定义任务相关的特征,同时过滤不必要的噪声特征,减少数据类型和维度,降低机器学习的难度,提供模型的准确度,它主要包括特征子集的产生,搜索和评估

一个好的特征选择,可以把那些对分类有价值的信息提取出来而过滤那些对分类没有价值的信息,可以利用**信息熵**和**信息增益**来进行**特征选择**

* 如果熵值很低,说明包含这个特征的数据只出现在少数类中,对分类的判断有价值.计算每个特征所对应的数据集之熵,可以按照熵值由低到高对特征进行排序,选择排列靠前的特征

* 如果基于某个特征的划分,所产生的信息增益越大,说明这个特征对于分类的判断越有价值.我们可以计算基于每个特征的划分,所产生的信息增益,按照信息增益由高到低对特征进行排序,挑选出排列靠前的特征

利用卡方检验进行特征选择,检验特征与分类两个变量是否独立:

* 如果两者独立,则特征和分类没有明显的相关性,反之两者具有较强的相关性
* 如果特征和分类的相关性很高,说明正向相关值远大于负向相关值,或负向相关值远大于正向相关值
* 如果特征与分类相关性很低,说明正向相关值和负向相关值很接近
* 按照卡方检验的值由高到低对特征进行排序,挑选出排列靠前的特征

## 特征变换

数值变换:使用统计中的数据分布,对连续型的数值进行转换,让多个特征的结合更有效

两种常见的特征变换方法:归一化和标准化

**归一化**:获取原始数据的最大值和最小值,然后把原始值线性变换到`[0,1]`之间

![](img\18.png)

* x为原始值,max为样本数据的最大值,min为样本数据的最小值
* 但是只考虑了最值,而最大值与最小值非常容易受到噪音数据的影响

**标准化**:基于正态分布的z分数标准化,假设数据呈现标准正太分布

![](img\19.png)

* x为原始值,μ为均差,σ为标准差
* 这样变换之后,高于平均数的分数会得到一个正的标准分,低于平均数的分数会得到一个负的标准分数,且转换后的数据符合标准状态分布,以0为均值,1为标准差
* 和归一化相比,标准化使用了数据是正态分布的假设,不容易受到过大或过小值得干扰

## 差异显著性检验

A/B测试:同一个目标制定两个或多个方案,让一部分用户使用A方案,另一部分用户使用B方案,记录下每个部分用户的使用情况,看哪个方案产生的结果更好

在A/B测试中,我们所得到得测试结果,实际上只是一种采样,不能简单地根据平均值来判断哪个方案更优

**显著性差异**:研究多组数据之间的差异,是由于数据分布导致还有由于采样的误差导致

* 有显著性差异:两个分布之间的差异
* 无显著性差异:采样引起的差异
* 显著性差异是指不同的组可能来自不同的数据分布,而显著差异是指差异的幅度很大,两者没有必然的联系

**统计假设检验**:事先对随机变量的参数或总体分布做出一个假设,然后利用样本信息来判断这个假设是否合理.这种假设称为**虚无假设**,通常记作H0.与虚无假设对立的假设称为**对立假设**,记作H1

* 先认为原假设H0成立,判断假设是否合理,若产生了小概率事件,则拒绝H0,接受H1,否则接受原假设
* 这个小概率事件的概率记为α,称为显著性水平,通常不超过0.05

**显著性检验**是统计假设检验的一种,它可以帮助我们判断多组数据之间的差异,是采样导致的偶然,还是由于不同数据分布导致的必然

* 假设多个数据之间没有差异(没有显著性)
* 如果样本发生的概率小于显著性水平α,则拒绝原假设,说明多个分布之间有差异,否则接受原假设

**P值**(Probability Value):H0假设为真时,样本出现的概率

* 如果P值很小,说明观测值与假设H0的期望值有很大的偏离,H0发生的概率很小,拒绝原假设,P值越小,越应该拒绝原假设
* 只要计算出P值,我们就能把P值和显著性水平α进行比较,从而决定是否接受原假设

## 方差分析

方差分析,也叫F检验,这种方法可以检验两种或者多组样本的均值是否具备显著性差异,它有四个前提假设

* 随机性:样本是随机采样的
* 独立性:来自不同组的样本是相互独立的
* 正态分布性:组内样本都是来自一个正态分布
* **方差齐性**:不同组的方差相等或相近

SST表示所有采样数据的因变量方差(Total Sum of Squares)

![](img\21.png)

SSM表时数据分布所引起的方差,称为模型平方和(Sum of Squares for Model)

![](img\22.png)

SSE表时采样引起的方差,称为误差平方和(Sum of Squares for Error)

![](img\23.png)

* SST=SSM+SSE
* 如果SSM占比更大,说明**因素**对因变量具有显著的影响
* 如果SSE占比更大,说明**采样误差**对因变量的差异具有显著的影响

![](img\20.png)

* `s`为水平(因素)的个数,`n`为所有样本的总数量,`s-1`为分布的自由度,`n-s`为误差的自由度
* F值越大,说明`SSM/(s-1)`越大,即因素对因变量具有显著影响,差异具**有显著性**
* F值越小,说明`SSE/(n-s)`越大,即采样误差对因变量的差异具有显著的影响,**差异没有显著性**

对于两个算法的比较,如a算法的测试结果的平均值要优于b算法,如果差异没有显著性,说明这个"优"的偶然性很大,并不意味着算法a比算法b更好

方差分析要求因变量属于正态分布,并且具有方差齐性,如果因变量的分布明显是非正态,或者方差的差异很显著,那么不能直接使用方差分析

* 对于方差不齐的情况,可以选择适当的函数对原始数据进行转换,或剔除某些数据,直到方差齐性变得显著
* 对于非正态分布的数据,我们可以使用非参数的分析,非参数检测是在总体的方差知道很少的情况下,利用样本数据对总体分布形态等进行推断的方法,常见的非参数检验包括二项分布检验,K-S检验,卡方检验等等

## 模型拟合

模型拟合:在监督式学习中训练一个模型,更学术地描述应该拟合一个模型,通过模型的假设和训练样本,推导出具体参数的过程

欠拟合:拟合得到的模型过于简单,和训练样本之间的误差非常大,这种拟合模型和训练样本之间的差异,称为偏差

过拟合:拟合得到的模型非常精细和复杂,和训练样本之间的误差非常小,这样的模型缺乏泛化的能力,无法很好地处理新的数据

![](img\24.png)

* 欠拟合产生的原因是特征维度过少,拟合的模型不够复杂,无法满足训练样本,最终导致误差较大,因此我们可以增加特征维度,让输入的训练样本具有更强的表达能力

* 过拟合的问题主要原因则是特征维度过多,导致拟合的模型过于完美地符合训练样本,但是无法适应测试样本或新数据.所以我们可以减少特征的维度,增加训练样本的数据量,并尽量保持训练数据和测试数据分布的一致性
  * 决策树算法容易导致过拟合,使用剪枝和随机森林来缓解过拟合
  * 当我们拥有大量的训练数据时,使用交叉验证的划分方式来保持训练数据和测试数据的一致性

## 概率统计总结

概率统计学习了很多概念

随机变量和它的概率分布体现了事物发生的不确定性,而条件概率、联合概率和边缘概率体现了多个随机变量之间的关系以及是不是相互独立，通过这三者的关系，可以推导出贝叶斯定理。

在贝叶斯定理和变量独立性假设的基础之上，推导出朴素贝叶斯算法中的公式，以及如何使用先验概率来预测后验概率。由于朴素贝叶斯假定多个变量之间相互独立，因此特别适合特征维度很多、特征向量和矩阵很稀疏的场景。基于词包方法的文本分类就是个非常典型的例子。

文本分类涉及了词与词之间相互独立的假设，然后延伸出多元文法，而多元文法也广泛应用在概率语言模型中。语言模型是马尔科夫模型的一种，而隐马尔科夫模型在马尔科夫模型的基础之上，提出了两层的结构，解决了我们无法直接观测到转移状态的问题。

由概率知识派生而来的信息论，也能帮助我们设计机器学习的算法，比如决策树和特征选择。统计中的数据分布可以让特征值转换更合理，而假设检验可以告诉我们哪些结论是更可靠的。

由于很多监督式学习都是基于概率统计的，概率和统计可以帮助机器学习中算法学习训练样本中的特征，并可以对新的数据进行预测，这就是模型拟合的过程。

# 线性代数

有些问题,我们不仅要关心单个变量之间的关系,还要进一步研究多个变量之间的关系,比如基于多个特征的信息检索和机器学习在研究多个变量之间关系的时候,线性代数成为了解决这类问题的有力工具,

## 基本概念

标量只是一个单独的数字,并且不能表示方向

向量代表一组数字,并且这些数字是有序排列,除了拥有数值的大小,还拥有方向

两个向量之间的加法,需要维度相同,对应的元素相加

向量之间的乘法默认是点乘,点乘的作用是把两个向量转换成了标量

矩阵由多个长度相等的向量组成

矩阵相乘:X的行向量与Y的列向量两两点乘

![](img\25.png)

矩阵转置:矩阵内的元素行索引和纵索引互换,就是原矩阵以对角线为轴进行翻转后的结果

![](img\26.png)

方阵:行数和列数相等

单位矩阵:所有沿主对角线的元素都是1,其它位置的所有元素都是0

逆矩阵:矩阵X的逆矩阵记作X^-1^,两者相乘的结果是单位矩阵

特征向量:我们使用向量来表示某个物体的特征,向量的每个元素代表一维特征,而元素的值代表了相应特征的值

矩阵的特征向量:矩阵几何意义是坐标的变换,矩阵的特征向量表示了它在空间中最主要的运动方向

## 向量空间模型

机器学习中的监督式学习,重点考察了特征和分类标签之间的关系,但是在信息检索和非监督学习中,我们更关注的是不同事物之间的相似程度,这就需要用到线性代数中的向量空间模型

F是一个实数域,Fn是由F上所有n维向量构成的集合,假设 V 是 Fn 的非零子集，如果对任意的向量 x、向量 y∈V，都有 (x+y)∈V，我们称为 V 对向量的加法封闭；对任意的标量 k∈V，向量 x∈V，都有 kx 属于 V，我们称 V 对标量与向量的乘法封闭。如果 V 满足向量的加法和乘法封闭性，我们就称 V 是 F 上的**向量空间**。

**向量之间的距离**:如果把一个向量想象为n为空间中的一个点,而向量空间中的两个向量的距离,就是这两个向量所对应的点之间的距离

曼哈顿距离:A点到B点有多条路径,但无论哪条,曼哈顿距离都是一样的

* ![](img\27.png)
* `n`表时向量维度,`xi`表示第一个向量的第`i`维元素的值,`yi`表示第二个向量第`i`维元素的值

欧式距离:欧几里得距离,指n维空间中两个点之间的真实距离

* ![](img\28.png)

切比雪夫距离:

* ![](img\29.png)

闵氏距离:前三个距离的通用表达形式

**向量的长度**:也叫向量的模,是向量所对应点到空间原点的距离,通常使用欧氏距离表示

**向量之间的夹角**:

* ![](img\30.png)
* 分子是两个向量的点乘,分母是两者长度的乘积
* 余弦值越大,说明两个向量夹角越小,两点相距越近,值越小,说明夹角越大,两点相距越远

**向量的空间模型**:向量的空间模型假设所有的对象都可以转化为向量,然后使用向量间的距离或者向量间的夹角余弦来表示两个对象之间的相似程度

* 夹角余弦的取值在-1到1之间,而且越大表示越相似,所以可以直接作为相似度的取值,欧式距离的范围可能很大,而且与相似度呈现反比关系,所以通常需要进行`1/(ED+1)`这种归一化
* 应用:信息检索领域,基于相似度的机器学习算法,如K近邻(KNN)分类,K均值(K-Means)聚类等

